# ===================================================================
#          Default Configuration for TransfQMix Experiment
# ===================================================================

# --- Top-level settings ---
experiment_name: "transfqmix_default_run"
seed: 42  # Seed for reproducibility, null for random
use_cuda: true # Use CUDA if available

# ===================================================================
#                       Environment Configuration
# ===================================================================
environment:
  episode_data_directory: "marl_framework/episode_data"
  specific_episode_file: "ep_SIM_all_features_R64_C64_S500_ID0000.npz" # Set to a filename like "ep_...npz" to run only one episode for debugging/eval

  # --- Grid and Agent Setup ---
  GRID_SIZE_R: 64
  GRID_SIZE_C: 64
  NUM_AGENTS: 3
  NUM_HEADINGS: 8 # 8 for N, NE, E, SE, S, SW, W, NW

  # --- Observation and Communication ---
  OBSERVATION_RADIUS_AGENTS: 10 # How many cells away an agent can see another agent
  COMMUNICATION_RADIUS_CELLS: 15 # How many cells away agents can exchange belief maps
  DIRECT_SENSING_MODE: "current_cell" # "current_cell", "surrounding_cells", or "none"

  # --- Feature and Normalization ---
  CNN_OUTPUT_FEATURE_DIM: 64 # Must match agent_nn.CNN_OUTPUT_FEATURE_DIM
  MAX_EXPECTED_CURRENT_MPS: 2.0 # For normalizing current vector features

  # ... existing yaml ...

# ===================================================================
#                       Environment Configuration
# ===================================================================
environment:
  # --- Reward Structure ---
  REWARD_SCALING_FACTOR: 100.0 # Multiplier for the change in IoU
  PENALTY_PER_STEP: 0 # Small penalty to encourage efficiency
  COLLISION_PENALTY: -10.0 # Penalty for agents moving to the same cell
  BOUNDARY_VIOLATION_PENALTY: -10.0 # Large penalty for trying to move off the map

  # --- Episode Control ---
  MAX_STEPS_PER_EPISODE: 400
  FALLBACK_ENV_TIME_STEP_HOURS: 0.1 # Used if not present in episode metadata

  # --- State Representation for Mixer (NEW FEATURES) ---
  INCLUDE_GLOBAL_BELIEF_IN_STATE: true # Add pooled shared belief map to mixer state
  INCLUDE_GROUND_TRUTH_IN_STATE: true  # Add pooled ground truth oil map to mixer state
  GLOBAL_BELIEF_POOLED_DIM: 8 # e.g., 8x8 pooled map from the main grid

# ===================================================================
#                    Agent Neural Network Configuration
# ===================================================================
agent_nn:
  # --- Belief Map Processing ---
  BELIEF_MAP_PROCESSOR: "maxpool" # 'cnn' or 'maxpool'. 'maxpool' is simpler and often more robust.
  BELIEF_MAP_POOL_DIM: 8 # Output dim of AdaptiveMaxPool2d (e.g., 8x8). Used only if processor is 'maxpool'.
  CNN_OUTPUT_FEATURE_DIM: 64 # Size of the feature vector from the belief map processor. Must match env config.

  # --- Agent Transformer ---
  AGENT_TRANSFORMER_EMBED_DIM: 64 # Internal embedding dimension
  AGENT_TRANSFORMER_NUM_HEADS: 4
  AGENT_TRANSFORMER_NUM_BLOCKS: 2
  AGENT_TRANSFORMER_FFN_DIM_MULTIPLIER: 4
  AGENT_TRANSFORMER_DROPOUT_RATE: 0.1

# ===================================================================
#                    Mixer Neural Network Configuration
# ===================================================================
mixer_nn:
  # --- Mixer Transformer ---
  MIXER_TRANSFORMER_EMBED_DIM: 64
  MIXER_TRANSFORMER_NUM_HEADS: 4
  MIXER_TRANSFORMER_NUM_BLOCKS: 2
  MIXER_TRANSFORMER_FFN_DIM_MULTIPLIER: 4
  MIXER_TRANSFORMER_DROPOUT_RATE: 0.1

  # --- Hypernetwork for Q-value Mixing MLP ---
  MIXER_MLP_HIDDEN_DIM: 64

# ===================================================================
#                         Training Configuration
# ===================================================================
training:
  num_training_episodes: 5000
  learning_starts_episodes: 10 # Start training after this many episodes of data collection
  batch_size: 32
  replay_buffer_capacity: 5000

  # --- Optimizer and Learning Control ---
  learning_rate: 0.0005
  gamma: 0.99 # Discount factor
  tau: 0.005 # Soft update parameter for target networks
  grad_norm_clip: 10.0

  # --- Exploration (Epsilon-Greedy) ---
  epsilon_start: 1.0
  epsilon_finish: 0.05
  epsilon_anneal_time: 100000 # Over how many total env steps to anneal epsilon

  # --- Network Updates and Evaluation ---
  target_update_interval_episodes: 10 # How often to soft-update target networks
  evaluation_interval_episodes: 100  # How often to run evaluation
  num_evaluation_episodes: 20 # Number of episodes to run for each evaluation
  save_model_interval_episodes: 500 # How often to save a model checkpoint

# ===================================================================
#                  Logging and Visualization Configuration
# ===================================================================
logging:
  log_dir: "logs"
  tb_log_dir: "runs"
  model_save_dir: "saved_models"
  visualization_gif_output_dir: "visualizations"
  log_level: "INFO" # DEBUG, INFO, WARNING, ERROR

  # --- GIF Generation ---
  visualization_enabled: true
  visualization_interval_eval_episodes: 5 # Create a GIF every Nth evaluation episode
  visualization_duration_per_frame_ms: 300 # Speed of the GIF frames in milliseconds